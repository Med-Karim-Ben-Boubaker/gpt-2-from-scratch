model:
  vocab_size: 50257
  context_length: 512
  emb_dim: 768
  n_heads: 12
  n_layers: 12
  drop_rate: 0.1
  qkv_bias: false

train:
  batch_size: 8
  lr: 0.0001
  weight_decay: 0.01
  num_epochs: 10
  eval_freq: 500
  eval_iter: 100
  grad_accum_steps: 4
  amp: true
  device: cuda
  seed: 123
  num_workers: 4
  warmup_steps: 2000
  min_lr: 1e-6
  betas: [0.9, 0.999]
  eps: 1e-8
  fused: true
  grad_clip_norm: 1.0
  # Checkpointing configuration
  # training_id: my_experiment  # Optional: auto-generated if not specified
  checkpoint_cadence: 1000  # Save checkpoint every N steps
  max_checkpoints: 5  # Keep only the most recent N checkpoints
